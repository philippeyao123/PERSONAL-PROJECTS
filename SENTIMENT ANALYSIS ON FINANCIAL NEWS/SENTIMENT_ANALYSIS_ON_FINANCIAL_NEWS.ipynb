{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yybLdZFy-eNz"
      },
      "source": [
        "# Import the libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YqblNWEp-c0Q"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import pandas as pd\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "import nltk\n",
        "from textblob import TextBlob\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import joblib\n",
        "import unittest\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFuvrYDL_olr"
      },
      "source": [
        "# Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yh0jFVDg-hmr"
      },
      "outputs": [],
      "source": [
        "# Configure logging\n",
        "logging.basicConfig(filename='sentiment_analysis.log', level=logging.INFO, format='%(asctime)s:%(levelname)s:%(message)s')\n",
        "\n",
        "# Function to mount Google Drive and load dataset\n",
        "def load_data_from_drive(file_path: str, encoding: str = 'ISO-8859-1') -> pd.DataFrame:\n",
        "    logging.info(f'Loading data from {file_path}')\n",
        "    drive.mount('/content/drive')\n",
        "    df = pd.read_csv(file_path, encoding=encoding)\n",
        "    logging.info('Data loaded successfully')\n",
        "    return df\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inMBNeUL_rQQ"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ScNt0Qie-54X"
      },
      "outputs": [],
      "source": [
        "# Function to preprocess the dataset\n",
        "def preprocess_data(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    logging.info('Preprocessing data')\n",
        "    df.columns = ['sentiment', 'news']\n",
        "    df = df.drop_duplicates()\n",
        "    df = df.dropna()\n",
        "    logging.info('Data preprocessing completed')\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vt4m2NDH_t-8"
      },
      "source": [
        "# Vader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "srdL7_ya-8y_"
      },
      "outputs": [],
      "source": [
        "# Function to perform VADER sentiment analysis\n",
        "def vader_sentiment_analysis(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    logging.info('Performing VADER sentiment analysis')\n",
        "    nltk.download('vader_lexicon')\n",
        "    sid = SentimentIntensityAnalyzer()\n",
        "\n",
        "    df['vader_compound'] = df['news'].apply(lambda x: sid.polarity_scores(x)['compound'])\n",
        "    df['vader_sentiment'] = df['vader_compound'].apply(classify_vader)\n",
        "    logging.info('VADER sentiment analysis completed')\n",
        "    return df\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "agbFKFF7_CqH"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Function to classify VADER compound scores\n",
        "def classify_vader(compound_score: float) -> str:\n",
        "    if compound_score >= 0.05:\n",
        "        return 'positive'\n",
        "    elif compound_score <= -0.05:\n",
        "        return 'negative'\n",
        "    else:\n",
        "        return 'neutral'\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SOLQJjP_xsB"
      },
      "source": [
        "# Textlob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rZSZrppp_Dxf"
      },
      "outputs": [],
      "source": [
        "# Function to perform TextBlob sentiment analysis\n",
        "def textblob_sentiment_analysis(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    logging.info('Performing TextBlob sentiment analysis')\n",
        "    df['textblob_sentiment'] = df['news'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
        "    df['textblob_sentiment_category'] = df['textblob_sentiment'].apply(classify_textblob)\n",
        "    logging.info('TextBlob sentiment analysis completed')\n",
        "    return df\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EsxDBzBh_K_C"
      },
      "outputs": [],
      "source": [
        "# Function to classify TextBlob polarity scores\n",
        "def classify_textblob(polarity_score: float) -> str:\n",
        "    if polarity_score > 0:\n",
        "        return 'positive'\n",
        "    elif polarity_score < 0:\n",
        "        return 'negative'\n",
        "    else:\n",
        "        return 'neutral'\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "im5ss3pl_0qu"
      },
      "source": [
        "# Confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wtiY4x8R_PPc"
      },
      "outputs": [],
      "source": [
        "# Function to visualize the confusion matrix\n",
        "def plot_confusion_matrix(y_true, y_pred, labels, model_name):\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.title(f'Confusion Matrix - {model_name}')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIqwQo2N_3w3"
      },
      "source": [
        "# Sentiment distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lw92KEK8_P-K"
      },
      "outputs": [],
      "source": [
        "# Function to visualize the distribution of sentiments\n",
        "def plot_sentiment_distribution(df: pd.DataFrame, sentiment_column: str, model_name: str):\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    sns.countplot(data=df, x=sentiment_column, order=['negative', 'neutral', 'positive'], palette='viridis')\n",
        "    plt.title(f'Sentiment Distribution - {model_name}')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zss8VYV__Tgo"
      },
      "outputs": [],
      "source": [
        "def evaluate_sentiment_analysis(df, model_name, prediction_column):\n",
        "    logging.info(f'Evaluating sentiment analysis performance for {model_name}')\n",
        "\n",
        "    # Convert predictions to string labels if they are numeric\n",
        "    if df[prediction_column].dtype != 'object':\n",
        "        df[prediction_column] = df[prediction_column].map({0: 'negative', 1: 'neutral', 2: 'positive'})  # Adjust mapping if needed\n",
        "\n",
        "    accuracy = accuracy_score(df['sentiment'], df[prediction_column])\n",
        "    classification_report_output = classification_report(df['sentiment'], df[prediction_column], target_names=['negative', 'neutral', 'positive'])\n",
        "\n",
        "    # Log and print the results\n",
        "    logging.info(f'Accuracy for {model_name}: {accuracy}')\n",
        "    logging.info(f'Classification Report for {model_name}:\\n{classification_report_output}')\n",
        "    print(f'--- {model_name} Performance ---')\n",
        "    print(f'Accuracy: {accuracy}')\n",
        "    print(f'Classification Report:\\n{classification_report_output}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eYe7oUw_7O1"
      },
      "source": [
        "# Training and evaluation models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rd1fxKSh_XUM"
      },
      "outputs": [],
      "source": [
        "# Function to train and evaluate machine learning models using Grid Search\n",
        "def train_and_evaluate_models(df: pd.DataFrame) -> None:\n",
        "    logging.info('Starting machine learning model training and evaluation')\n",
        "\n",
        "    # Step 1: TF-IDF Vectorization\n",
        "    tfidf = TfidfVectorizer(max_features=5000)\n",
        "    X = tfidf.fit_transform(df['news']).toarray()\n",
        "    y = df['sentiment'].map({'negative': 0, 'neutral': 1, 'positive': 2})\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Define models and parameters for Grid Search\n",
        "    models = {\n",
        "        'Logistic Regression': LogisticRegression(),\n",
        "        'SVM': SVC(),\n",
        "        'Random Forest': RandomForestClassifier()\n",
        "    }\n",
        "\n",
        "    params = {\n",
        "        'Logistic Regression': {'C': [0.1, 1, 10], 'max_iter': [100, 200]},\n",
        "        'SVM': {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']},\n",
        "        'Random Forest': {'n_estimators': [50, 100, 200], 'max_depth': [10, 20, None]}\n",
        "    }\n",
        "\n",
        "    best_models = {}\n",
        "\n",
        "    for name, model in models.items():\n",
        "        logging.info(f'Performing Grid Search for {name}')\n",
        "        clf = GridSearchCV(model, params[name], cv=5)\n",
        "        clf.fit(X_train, y_train)\n",
        "        best_models[name] = clf.best_estimator_\n",
        "        preds = clf.predict(X)\n",
        "        logging.info(f'Best parameters for {name}: {clf.best_params_}')\n",
        "        print(f'---{name}---')\n",
        "        print(f'Best parameters: {clf.best_params_}')\n",
        "        evaluate_sentiment_analysis(df.assign(predicted=preds), model_name=name, prediction_column='predicted')\n",
        "\n",
        "    # Save the best model and the TF-IDF vectorizer\n",
        "    logging.info('Saving the best model and TF-IDF vectorizer')\n",
        "    best_model = best_models['Logistic Regression']  # You can change this based on the performance\n",
        "    joblib.dump(best_model, 'best_model.pkl')\n",
        "    joblib.dump(tfidf, 'tfidf_vectorizer.pkl')\n",
        "    logging.info('Model training and evaluation completed')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_esrX0pD_-wE"
      },
      "source": [
        "# Performance analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6s4ReTs4_caH"
      },
      "outputs": [],
      "source": [
        "# Function to compare the performance of VADER, TextBlob, and ML models\n",
        "def compare_models(df: pd.DataFrame) -> None:\n",
        "    # Evaluate VADER\n",
        "    evaluate_sentiment_analysis(df, model_name='VADER', prediction_column='vader_sentiment')\n",
        "\n",
        "    # Evaluate TextBlob\n",
        "    evaluate_sentiment_analysis(df, model_name='TextBlob', prediction_column='textblob_sentiment_category')\n",
        "\n",
        "    # Train and Evaluate ML Models\n",
        "    train_and_evaluate_models(df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEIxnFNdADKs"
      },
      "source": [
        "# Unit tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tD-qdW6d_gK8"
      },
      "outputs": [],
      "source": [
        "# Unit tests for the sentiment analysis functions\n",
        "class TestSentimentAnalysis(unittest.TestCase):\n",
        "    def setUp(self):\n",
        "        # Sample data for testing\n",
        "        self.df = pd.DataFrame({\n",
        "            'sentiment': ['positive', 'negative', 'neutral'],\n",
        "            'news': [\n",
        "                'This is a great day for the market!',\n",
        "                'The market is crashing, it is a bad day.',\n",
        "                'The market is stable with no major changes.'\n",
        "            ]\n",
        "        })\n",
        "\n",
        "    def test_classify_vader(self):\n",
        "        self.assertEqual(classify_vader(0.1), 'positive')\n",
        "        self.assertEqual(classify_vader(-0.1), 'negative')\n",
        "        self.assertEqual(classify_vader(0), 'neutral')\n",
        "\n",
        "    def test_classify_textblob(self):\n",
        "        self.assertEqual(classify_textblob(0.1), 'positive')\n",
        "        self.assertEqual(classify_textblob(-0.1), 'negative')\n",
        "        self.assertEqual(classify_textblob(0), 'neutral')\n",
        "\n",
        "    def test_vader_sentiment_analysis(self):\n",
        "        df_result = vader_sentiment_analysis(self.df.copy())\n",
        "        self.assertIn('vader_compound', df_result.columns)\n",
        "        self.assertIn('vader_sentiment', df_result.columns)\n",
        "        self.assertEqual(df_result.shape[0], 3)  # Ensure the number of rows is unchanged\n",
        "        self.assertTrue(all(df_result['vader_sentiment'].isin(['positive', 'negative', 'neutral'])))\n",
        "\n",
        "    def test_textblob_sentiment_analysis(self):\n",
        "        df_result = textblob_sentiment_analysis(self.df.copy())\n",
        "        self.assertIn('textblob_sentiment', df_result.columns)\n",
        "        self.assertIn('textblob_sentiment_category', df_result.columns)\n",
        "        self.assertEqual(df_result.shape[0], 3)  # Ensure the number of rows is unchanged\n",
        "        self.assertTrue(all(df_result['textblob_sentiment_category'].isin(['positive', 'negative', 'neutral'])))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tx4seM1NAF8W"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vpMlC7Zt_kTU",
        "outputId": "5de14864-0f35-4846-cc5f-0efec49dfe99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- VADER Performance ---\n",
            "Accuracy: 0.5435007232899359\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.41      0.30      0.34       604\n",
            "     neutral       0.74      0.52      0.61      2872\n",
            "    positive       0.40      0.71      0.51      1363\n",
            "\n",
            "    accuracy                           0.54      4839\n",
            "   macro avg       0.52      0.51      0.49      4839\n",
            "weighted avg       0.60      0.54      0.55      4839\n",
            "\n",
            "--- TextBlob Performance ---\n",
            "Accuracy: 0.4908038851002273\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.30      0.38      0.34       604\n",
            "     neutral       0.63      0.54      0.58      2872\n",
            "    positive       0.36      0.43      0.39      1363\n",
            "\n",
            "    accuracy                           0.49      4839\n",
            "   macro avg       0.43      0.45      0.44      4839\n",
            "weighted avg       0.52      0.49      0.50      4839\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---Logistic Regression---\n",
            "Best parameters: {'C': 10, 'max_iter': 100}\n",
            "--- Logistic Regression Performance ---\n",
            "Accuracy: 0.9497830130192189\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.97      0.90      0.93       604\n",
            "     neutral       0.95      0.98      0.96      2872\n",
            "    positive       0.95      0.91      0.93      1363\n",
            "\n",
            "    accuracy                           0.95      4839\n",
            "   macro avg       0.95      0.93      0.94      4839\n",
            "weighted avg       0.95      0.95      0.95      4839\n",
            "\n",
            "---SVM---\n",
            "Best parameters: {'C': 10, 'kernel': 'rbf'}\n",
            "--- SVM Performance ---\n",
            "Accuracy: 0.9532961355651994\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.98      0.90      0.94       604\n",
            "     neutral       0.94      0.99      0.96      2872\n",
            "    positive       0.97      0.91      0.93      1363\n",
            "\n",
            "    accuracy                           0.95      4839\n",
            "   macro avg       0.96      0.93      0.95      4839\n",
            "weighted avg       0.95      0.95      0.95      4839\n",
            "\n",
            "---Random Forest---\n",
            "Best parameters: {'max_depth': None, 'n_estimators': 100}\n",
            "--- Random Forest Performance ---\n",
            "Accuracy: 0.9479231246125233\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.99      0.88      0.93       604\n",
            "     neutral       0.93      0.99      0.96      2872\n",
            "    positive       0.97      0.88      0.92      1363\n",
            "\n",
            "    accuracy                           0.95      4839\n",
            "   macro avg       0.96      0.92      0.94      4839\n",
            "weighted avg       0.95      0.95      0.95      4839\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 0 tests in 0.000s\n",
            "\n",
            "OK\n"
          ]
        }
      ],
      "source": [
        "# Main function to run all steps\n",
        "def main():\n",
        "    logging.info('Starting the sentiment analysis pipeline')\n",
        "\n",
        "    file_path = '/content/drive/MyDrive/all-data.csv'\n",
        "    df = load_data_from_drive(file_path)\n",
        "    df = preprocess_data(df)\n",
        "\n",
        "    # VADER Analysis\n",
        "    df = vader_sentiment_analysis(df)\n",
        "\n",
        "    # TextBlob Analysis\n",
        "    df = textblob_sentiment_analysis(df)\n",
        "\n",
        "    # Compare Models\n",
        "    compare_models(df)\n",
        "\n",
        "    logging.info('Sentiment analysis pipeline completed successfully')\n",
        "\n",
        "# Run the main function\n",
        "if __name__ == \"__main__\":\n",
        "    # Run the main pipeline\n",
        "    main()\n",
        "\n",
        "    # Run unit tests\n",
        "    logging.info('Running unit tests')\n",
        "    unittest.main(argv=[''], verbosity=2, exit=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}