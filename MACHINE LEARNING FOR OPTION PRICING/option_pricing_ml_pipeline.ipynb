{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36d8cc69",
   "metadata": {},
   "source": [
    "# Machine Learning Option Pricing Pipeline\n",
    "\n",
    "This notebook demonstrates how to generate synthetic European call option data using the Black–Scholes formula, train several machine‑learning models to approximate option prices, and discuss how sequence models (LSTM/GRU) could be applied when sequential features are available.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efeb4fe",
   "metadata": {},
   "source": [
    "## Theoretical background\n",
    "\n",
    "Under the Black–Scholes model, the underlying stock price $S_t$ follows geometric Brownian motion: $$\\mathrm{d}S_t = \\mu S_t\\,\\mathrm{d}t + \\sigma S_t\\,\\mathrm{d}W_t$$, which has the solution $S_t = S_0 \\exp[(\\mu - \\sigma^2/2)t + \\sigma W_t]$.  The price of a European call option with strike $K$, maturity $t$ and risk‑free rate $r$ is given by\n",
    "\n",
    "$$C = S N(d_1) - K e^{-r t} N(d_2),$$\n",
    "\n",
    "where $d_1 = \f",
    "rac{\\ln(S/K) + (r + \\sigma^2/2)t}{\\sigma\\sqrt{t}}$ and $d_2 = d_1 - \\sigma \\sqrt{t}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5af694",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import norm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Fix random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Black–Scholes call option price function\n",
    "def black_scholes_call_price(S, K, t, r, sigma):\n",
    "    d1 = (np.log(S / K) + (r + 0.5 * sigma**2) * t) / (sigma * np.sqrt(t))\n",
    "    d2 = d1 - sigma * np.sqrt(t)\n",
    "    return S * norm.cdf(d1) - K * np.exp(-r * t) * norm.cdf(d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40676549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data\n",
    "n_samples = 5000\n",
    "\n",
    "S = np.random.uniform(20, 150, n_samples)      # current stock prices\n",
    "K = np.random.uniform(10, 200, n_samples)      # strike prices\n",
    "t = np.random.uniform(0.1, 2.0, n_samples)     # time to maturity in years\n",
    "sigma = np.random.uniform(0.1, 0.6, n_samples) # volatility (10% to 60%)\n",
    "r = np.random.uniform(0.01, 0.05, n_samples)   # risk‑free rate (1% to 5%)\n",
    "\n",
    "prices = black_scholes_call_price(S, K, t, r, sigma)\n",
    "\n",
    "# Assemble data frame\n",
    "data = pd.DataFrame({\n",
    "    'S': S,\n",
    "    'K': K,\n",
    "    't': t,\n",
    "    'sigma': sigma,\n",
    "    'r': r,\n",
    "    'price': prices\n",
    "})\n",
    "\n",
    "# Split into training and test sets\n",
    "X = data[['S', 'K', 't', 'sigma', 'r']]\n",
    "y = data['price']\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175b73a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Linear Regression model\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(train_X, train_y)\n",
    "pred_lin = lin_reg.predict(test_X)\n",
    "\n",
    "# Train Random Forest Regressor\n",
    "rf_reg = RandomForestRegressor(n_estimators=200, random_state=42)\n",
    "rf_reg.fit(train_X, train_y)\n",
    "pred_rf = rf_reg.predict(test_X)\n",
    "\n",
    "# Train MLP Regressor (neural network)\n",
    "mlp_reg = MLPRegressor(hidden_layer_sizes=(100,), activation='relu', solver='adam', random_state=42, max_iter=500)\n",
    "mlp_reg.fit(train_X, train_y)\n",
    "pred_mlp = mlp_reg.predict(test_X)\n",
    "\n",
    "# Evaluate models\n",
    "models = {'Linear Regression': pred_lin, 'Random Forest': pred_rf, 'MLP Regressor': pred_mlp}\n",
    "for name, pred in models.items():\n",
    "    rmse = mean_squared_error(test_y, pred, squared=False)\n",
    "    mae = mean_absolute_error(test_y, pred)\n",
    "    r2 = r2_score(test_y, pred)\n",
    "    print(f\"{name}: RMSE={rmse:.4f}, MAE={mae:.4f}, R2={r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2458ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions vs true prices for a sample of 200 points\n",
    "sample_idx = np.random.choice(len(test_y), 200, replace=False)\n",
    "true_sample = test_y.iloc[sample_idx].values\n",
    "plt.figure(figsize=(10,5))\n",
    "for name, pred in models.items():\n",
    "    pred_sample = pred[sample_idx]\n",
    "    plt.scatter(true_sample, pred_sample, label=name, alpha=0.7)\n",
    "plt.xlabel('True Price')\n",
    "plt.ylabel('Predicted Price')\n",
    "plt.title('Predicted vs True Option Prices (sample 200)')\n",
    "plt.legend()\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba8a39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate GBM paths to visualise underlying dynamics\n",
    "n_paths = 10\n",
    "n_steps = 252  # daily steps for one year\n",
    "delta_t = 1/252\n",
    "time = np.linspace(0, 1, n_steps)\n",
    "mu = 0.05\n",
    "vol = 0.2\n",
    "S0 = 100\n",
    "\n",
    "gbm_paths = np.zeros((n_paths, n_steps))\n",
    "gbm_paths[:,0] = S0\n",
    "for i in range(1, n_steps):\n",
    "    z = np.random.normal(size=n_paths)\n",
    "    gbm_paths[:, i] = gbm_paths[:, i-1] * np.exp((mu - 0.5 * vol**2) * delta_t + vol * np.sqrt(delta_t) * z)\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "for j in range(n_paths):\n",
    "    plt.plot(time, gbm_paths[j], linewidth=1)\n",
    "plt.xlabel('Time (years)')\n",
    "plt.ylabel('Stock Price')\n",
    "plt.title('Sample Geometric Brownian Motion Paths')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56281f45",
   "metadata": {},
   "source": [
    "## LSTM/GRU skeleton code\n",
    "\n",
    "The following illustrates how to build an LSTM‑based regressor using PyTorch.  It accepts an input tensor of shape `(batch_size, sequence_length, n_features)` and outputs a single price prediction per sequence:\n",
    "\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "    class LSTMRegressor(nn.Module):\n",
    "        def __init__(self, input_dim: int, hidden_dim: int = 64, num_layers: int = 2):\n",
    "            super().__init__()\n",
    "            self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "            self.fc = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "        def forward(self, x):\n",
    "            # x: [batch_size, seq_len, input_dim]\n",
    "            out, _ = self.lstm(x)\n",
    "            out = out[:, -1, :]\n",
    "            return self.fc(out).squeeze(1)\n",
    "\n",
    "    # Example usage (assumes train_X is a tensor of shape [n_samples, seq_len, n_features])\n",
    "    model = LSTMRegressor(input_dim=train_X.size(2))\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    train_dataset = TensorDataset(train_X, train_y)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "    for epoch in range(50):\n",
    "        model.train()\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(X_batch)\n",
    "            loss = criterion(pred, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "To construct a GRU‑based model simply replace `nn.LSTM` with `nn.GRU`.  In practice, one would use actual historical price sequences and other time‑varying features as inputs.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
