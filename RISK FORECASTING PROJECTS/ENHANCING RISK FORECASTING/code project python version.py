# -*- coding: utf-8 -*-
"""CODEPROJECT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rWVPLJD3xoDzEA41vx7cAjI_mKsmrQ2g

### Assessed Homework Solution - Enhancing Financial Risk Forecasting: Machine Learning with and without Macroeconomic Variables Across Market Regimes
### Candidate Number: 37279
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from statsmodels.tsa.seasonal import seasonal_decompose
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.model_selection import TimeSeriesSplit
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, GRU, Dense
from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator
from statsmodels.tsa.regime_switching.markov_regression import MarkovRegression

dataset_path = '/content/drive/MyDrive/RESEARCHPROJECTDATA.csv'
data = pd.read_csv(dataset_path)
data.head()

data.columns

data.describe()

"""## Data Preprocessing and feature engineering"""

# 1. Handle Missing Values
data = data.dropna(subset=['PRC', 'RET', 'BID', 'ASK', 'cpiret', 'UNRATE', 'GDP'])

# 2. Date Formatting
data['date'] = pd.to_datetime(data['date'], format='%Y-%m-%d')

# 3. Remove Duplicates
data = data.drop_duplicates(subset=['PERMNO', 'date'])

# 4. Filter Relevant Data
selected_tickers = ['AAPL', 'C', 'F']
data = data[data['TICKER'].isin(selected_tickers)]

# 5. Calculate Log Returns
data['LogReturn'] = data.groupby('TICKER')['PRC'].transform(lambda x: np.log(x / x.shift(1)))

# 6. Normalize Macroeconomic Variables
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
data[['cpiret_norm', 'UNRATE_norm', 'GDP_norm']] = scaler.fit_transform(
    data[['cpiret', 'UNRATE', 'GDP']]
)

# 7. Feature Engineering
# Lagged Features
for lag in range(1, 4):
    data[f'RET_Lag{lag}'] = data.groupby('TICKER')['RET'].transform(lambda x: x.shift(lag))
    data[f'cpiret_Lag{lag}'] = data['cpiret'].shift(lag)
    data[f'UNRATE_Lag{lag}'] = data['UNRATE'].shift(lag)

# Rolling Volatility
data['Volatility'] = data.groupby('TICKER')['RET'].transform(lambda x: x.rolling(window=20).std())

# Moving Averages
data['MA_20'] = data.groupby('TICKER')['PRC'].transform(lambda x: x.rolling(window=20).mean())
data['MA_50'] = data.groupby('TICKER')['PRC'].transform(lambda x: x.rolling(window=50).mean())

# MACD
data['EMA_12'] = data.groupby('TICKER')['PRC'].transform(lambda x: x.ewm(span=12, adjust=False).mean())
data['EMA_26'] = data.groupby('TICKER')['PRC'].transform(lambda x: x.ewm(span=26, adjust=False).mean())
data['MACD'] = data['EMA_12'] - data['EMA_26']

# RSI
data['Delta'] = data.groupby('TICKER')['PRC'].diff()
data['Gain'] = data['Delta'].where(data['Delta'] > 0, 0).groupby(data['TICKER']).transform(lambda x: x.rolling(window=14).mean())
data['Loss'] = -data['Delta'].where(data['Delta'] < 0, 0).groupby(data['TICKER']).transform(lambda x: x.rolling(window=14).mean())
data['RSI'] = 100 - (100 / (1 + data['Gain'] / data['Loss']))

# Bid-Ask Spread
data['Spread'] = data['ASK'] - data['BID']

# Market Regime Classification
data['CumulativeReturn'] = data.groupby('TICKER')['RET'].transform(lambda x: x.rolling(window=30).sum())
data['MarketRegime'] = np.where(
    data['CumulativeReturn'] > 0, 'Bull',
    np.where(data['CumulativeReturn'] < -0.05, 'Bear', 'Crisis')
)

# Interaction Features
data['RET_cpiret'] = data['RET'] * data['cpiret']
data['RET_UNRATE'] = data['RET'] * data['UNRATE']

# Drop rows with NaN values introduced during feature engineering
data = data.dropna()
data

# Function to plot rolling volatility and returns for each ticker
def plot_volatility_and_returns(data, tickers, rolling_col='Volatility', return_col='LogReturn'):

    for ticker in tickers:
        ticker_data = data[data['TICKER'] == ticker]

        # Plot Rolling Volatility
        plt.figure(figsize=(14, 6))
        plt.plot(ticker_data['date'], ticker_data[rolling_col], label='Rolling Volatility', color='blue')
        plt.title(f"{ticker} - Rolling Volatility Over Time")
        plt.xlabel("Date")
        plt.ylabel("Volatility")
        plt.legend()
        plt.grid(True)
        plt.show()

        # Plot Log Returns
        plt.figure(figsize=(14, 6))
        plt.plot(ticker_data['date'], ticker_data[return_col], label='Log Returns', color='green')
        plt.axhline(0, color='red', linestyle='--', linewidth=0.8)
        plt.title(f"{ticker} - Log Returns Over Time")
        plt.xlabel("Date")
        plt.ylabel("Log Returns")
        plt.legend()
        plt.grid(True)
        plt.show()

# List of tickers to visualize
selected_tickers = ['AAPL', 'C', 'F']

# Call the function
plot_volatility_and_returns(data, selected_tickers)

data.columns

"""## Data visualization

### Correlation matrix
"""

# 1. Correlation Matrix (Numerical Variables Only)
numerical_data = data.select_dtypes(include=[np.number])
correlation_matrix = numerical_data.corr()
mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))
plt.figure(figsize=(18, 15))
sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)
plt.title('Correlation Matrix (Numerical Variables Only - Lower Triangle)')
plt.show()

"""### Time-Series comparison: MACD and RSI"""

# 3. Time-Series Comparison: RSI
for ticker in data['TICKER'].unique():
    ticker_data = data[data['TICKER'] == ticker]
    plt.figure(figsize=(12, 6))
    plt.plot(ticker_data['date'], ticker_data['RSI'], label='RSI', color='orange')
    plt.axhline(y=70, color='red', linestyle='--', label='RSI Overbought')
    plt.axhline(y=30, color='green', linestyle='--', label='RSI Oversold')
    plt.title(f'RSI Over Time for {ticker}')
    plt.xlabel('Date')
    plt.ylabel('Indicator Value')
    plt.legend()
    plt.show()

"""### Closing Price and Moving Averages Over Time"""

# 4. Closing Price and Moving Averages Over Time
for ticker in data['TICKER'].unique():
    ticker_data = data[data['TICKER'] == ticker]
    plt.figure(figsize=(12, 6))
    plt.plot(ticker_data['date'], ticker_data['PRC'], label='Closing Price', color='red')
    plt.plot(ticker_data['date'], ticker_data['MA_20'], label='20-Day MA', color='green')
    plt.plot(ticker_data['date'], ticker_data['MA_50'], label='50-Day MA', color='orange')
    plt.title(f'Closing Price and Moving Averages for {ticker}')
    plt.xlabel('Date')
    plt.ylabel('Price')
    plt.legend()
    plt.show()

"""## Model Implementation and Performance Analysis"""

data.columns

"""## Model Performance and analysis of the all the dataset"""

# Step 1: Regime Classification
# Apply a threshold-based approach to classify market regimes (bull, bear, crisis) based on returns and volatility.
data['MarketRegime'] = np.where(
    data['RET'] > 0.02, 'Bull',
    np.where(data['RET'] < -0.02, 'Bear', 'Crisis')
)
data = pd.get_dummies(data, columns=['MarketRegime'], drop_first=True)


# Prepare data
data['date'] = pd.to_datetime(data['date'])
data.dropna(inplace=True)
sequence_length = 10
n_splits = 2
target = 'PRC'
features = data.drop(columns=[target, 'date', 'TICKER']).columns

def calculate_metrics(y_true, y_pred, n, p):
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_true, y_pred)
    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100
    r2 = r2_score(y_true, y_pred)
    adj_r2 = 1 - ((1 - r2) * (n - 1) / (n - p - 1))
    return mse, rmse, mae, mape, r2, adj_r2

def calculate_var_es(predictions, alpha=0.95):
    var = np.percentile(predictions, (1 - alpha) * 100)
    es = predictions[predictions <= var].mean()
    return var, es

ts_results = {}
for ticker in data['TICKER'].unique():
    ticker_data = data[data['TICKER'] == ticker]
    tscv = TimeSeriesSplit(n_splits=n_splits)
    results_dict = {
        'LSTM': [],
        'GRU': [],
        'GBM': []
    }

    # Cross-validation for LSTM and GRU
    for model_name in ['LSTM', 'GRU']:
        for fold, (train_index, test_index) in enumerate(tscv.split(ticker_data[target].values)):
            print(f"{ticker} - {model_name}: Processing fold {fold+1}/{n_splits}")

            train, test = ticker_data[target].values[train_index], ticker_data[target].values[test_index]

            generator = TimeseriesGenerator(train, train, length=sequence_length, batch_size=1)
            X_train, y_train = [], []
            for i in range(len(generator)):
                x_, y_ = generator[i]
                X_train.append(x_)
                y_train.append(y_)
            X_train = np.array(X_train).squeeze()
            y_train = np.array(y_train)

            generator = TimeseriesGenerator(test, test, length=sequence_length, batch_size=1)
            X_test, y_test = [], []
            for i in range(len(generator)):
                x_, y_ = generator[i]
                X_test.append(x_)
                y_test.append(y_)
            X_test = np.array(X_test).squeeze()
            y_test = np.array(y_test)

            X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))
            X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))

            # Build and train the model
            if model_name == 'LSTM':
                model = Sequential([
                    LSTM(50, activation='relu', input_shape=(sequence_length, 1)),
                    Dense(1)
                ])
            else:
                model = Sequential([
                    GRU(50, activation='relu', input_shape=(sequence_length, 1)),
                    Dense(1)
                ])

            model.compile(optimizer='adam', loss='mse')
            model.fit(X_train, y_train, epochs=20, verbose=1)

            # Make predictions
            predictions = model.predict(X_test).flatten()

            # Calculate metrics
            mse, rmse, mae, mape, r2, adj_r2 = calculate_metrics(y_test, predictions, len(y_test), X_test.shape[1])
            var_95, es_95 = calculate_var_es(predictions, alpha=0.95)
            var_99, es_99 = calculate_var_es(predictions, alpha=0.99)

            results_dict[model_name].append((mse, rmse, mae, mape, r2, adj_r2, var_95, es_95, var_99, es_99))

    # Cross-validation for GBM
    for fold, (train_index, test_index) in enumerate(tscv.split(ticker_data[target].values)):
        print(f"{ticker} - GBM: Processing fold {fold+1}/{n_splits}")

        X_train, X_test = ticker_data.iloc[train_index][features], ticker_data.iloc[test_index][features]
        y_train, y_test = ticker_data.iloc[train_index][target], ticker_data.iloc[test_index][target]

        gbm = GradientBoostingRegressor()
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
        gbm.fit(X_train_scaled, y_train)
        gbm_pred = gbm.predict(X_test_scaled)

        mse, rmse, mae, mape, r2, adj_r2 = calculate_metrics(y_test, gbm_pred, len(y_test), X_test_scaled.shape[1])
        var_95, es_95 = calculate_var_es(gbm_pred, alpha=0.95)
        var_99, es_99 = calculate_var_es(gbm_pred, alpha=0.99)

        results_dict['GBM'].append((mse, rmse, mae, mape, r2, adj_r2, var_95, es_95, var_99, es_99))

    ts_results[ticker] = results_dict

# Evaluation Metrics
# Results per ticker:
for ticker, results_dict in ts_results.items():
    print(f"{ticker} Results:")
    for model, metrics in results_dict.items():
        avg_metrics = np.mean(metrics, axis=0)
        print(f"{model}: MSE={avg_metrics[0]:.2f}, RMSE={avg_metrics[1]:.2f}, MAE={avg_metrics[2]:.2f}, MAPE={avg_metrics[3]:.2f}%, R²={avg_metrics[4]:.2f}, Adj R²={avg_metrics[5]:.2f}, VaR_95={avg_metrics[6]:.2f}, ES_95={avg_metrics[7]:.2f}, VaR_99={avg_metrics[8]:.2f}, ES_99={avg_metrics[9]:.2f}")

"""## Model Performance and Analysis without macroeconomic variables"""

# Prepare data
data['date'] = pd.to_datetime(data['date'])
data.dropna(inplace=True)

# Target and features
target = 'PRC'
exclude_features = ['cpiret', 'UNRATE', 'GDP', 'cpiret_Lag1', 'UNRATE_Lag1', 'GDP_Lag1',
                    'cpiret_Lag2', 'UNRATE_Lag2', 'GDP_Lag2', 'cpiret_Lag3', 'UNRATE_Lag3', 'GDP_Lag3']
features = [col for col in data.columns if col not in exclude_features + [target, 'date', 'TICKER']]
print(f"Features used for training: {features}")

# Time-series split and model training using features
sequence_length = 10
n_splits = 2

# Time-series cross-validation and evaluation
ts_results = {}
for ticker in data['TICKER'].unique():
    ticker_data = data[data['TICKER'] == ticker]
    tscv = TimeSeriesSplit(n_splits=n_splits)
    results_dict = {
        'LSTM': [],
        'GRU': [],
        'GBM': []
    }

    # Cross-validation for LSTM and GRU
    for model_name in ['LSTM', 'GRU']:
        for fold, (train_index, test_index) in enumerate(tscv.split(ticker_data[target].values)):
            print(f"{ticker} - {model_name}: Processing fold {fold+1}/{n_splits}")

            train, test = ticker_data[target].values[train_index], ticker_data[target].values[test_index]

            generator = TimeseriesGenerator(train, train, length=sequence_length, batch_size=1)
            X_train, y_train = [], []
            for i in range(len(generator)):
                x_, y_ = generator[i]
                X_train.append(x_)
                y_train.append(y_)
            X_train = np.array(X_train).squeeze()
            y_train = np.array(y_train)

            generator = TimeseriesGenerator(test, test, length=sequence_length, batch_size=1)
            X_test, y_test = [], []
            for i in range(len(generator)):
                x_, y_ = generator[i]
                X_test.append(x_)
                y_test.append(y_)
            X_test = np.array(X_test).squeeze()
            y_test = np.array(y_test)

            X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))
            X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))

            # Build and train the model
            if model_name == 'LSTM':
                model = Sequential([
                    LSTM(50, activation='relu', input_shape=(sequence_length, 1)),
                    Dense(1)
                ])
            else:
                model = Sequential([
                    GRU(50, activation='relu', input_shape=(sequence_length, 1)),
                    Dense(1)
                ])

            model.compile(optimizer='adam', loss='mse')
            model.fit(X_train, y_train, epochs=20, verbose=1)

            # Make predictions
            predictions = model.predict(X_test).flatten()

            # Calculate metrics
            mse, rmse, mae, mape, r2, adj_r2 = calculate_metrics(y_test, predictions, len(y_test), X_test.shape[1])
            var_95, es_95 = calculate_var_es(predictions, alpha=0.95)
            var_99, es_99 = calculate_var_es(predictions, alpha=0.99)

            results_dict[model_name].append((mse, rmse, mae, mape, r2, adj_r2, var_95, es_95, var_99, es_99))

    # Cross-validation for GBM
    for fold, (train_index, test_index) in enumerate(tscv.split(ticker_data[target].values)):
        print(f"{ticker} - GBM: Processing fold {fold+1}/{n_splits}")

        X_train, X_test = ticker_data.iloc[train_index][features], ticker_data.iloc[test_index][features]
        y_train, y_test = ticker_data.iloc[train_index][target], ticker_data.iloc[test_index][target]

        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
        gbm = GradientBoostingRegressor()
        gbm.fit(X_train_scaled, y_train)
        gbm_pred = gbm.predict(X_test_scaled)

        mse, rmse, mae, mape, r2, adj_r2 = calculate_metrics(y_test, gbm_pred, len(y_test), X_test_scaled.shape[1])
        var_95, es_95 = calculate_var_es(gbm_pred, alpha=0.95)
        var_99, es_99 = calculate_var_es(gbm_pred, alpha=0.99)

        results_dict['GBM'].append((mse, rmse, mae, mape, r2, adj_r2, var_95, es_95, var_99, es_99))

    ts_results[ticker] = results_dict

# Evaluation Metrics
# Results per ticker:
for ticker, results_dict in ts_results.items():
    print(f"{ticker} Results:")
    for model, metrics in results_dict.items():
        avg_metrics = np.mean(metrics, axis=0)
        print(f"{model}: MSE={avg_metrics[0]:.2f}, RMSE={avg_metrics[1]:.2f}, MAE={avg_metrics[2]:.2f}, MAPE={avg_metrics[3]:.2f}%, R²={avg_metrics[4]:.2f}, Adj R²={avg_metrics[5]:.2f}, VaR_95={avg_metrics[6]:.2f}, ES_95={avg_metrics[7]:.2f}, VaR_99={avg_metrics[8]:.2f}, ES_99={avg_metrics[9]:.2f}")

"""## Regime Switch all dataset"""

data['MarketRegime'] = np.where(
    data['RET'] > 0.02, 'Bull',
    np.where(data['RET'] < -0.02, 'Bear', 'Crisis')
)

# Fit Markov Switching Model on returns
markov_model = MarkovRegression(data['RET'], k_regimes=3, trend='c', switching_variance=True)
markov_results = markov_model.fit()

# Add predicted regimes to the dataset
data['MarkovRegime'] = markov_results.predict()

# Segment Data by Regime and Evaluate Model Performance
regime_results = {}

for regime in ['Bull', 'Bear', 'Crisis']:
    print(f"Evaluating models for {regime} regime")
    regime_data = data[data['MarketRegime'] == regime]

    # Prepare data for modeling
    tscv = TimeSeriesSplit(n_splits=n_splits)
    results_dict = {'LSTM': [], 'GRU': [], 'GBM': []}

    for model_name in ['LSTM', 'GRU']:
        for fold, (train_index, test_index) in enumerate(tscv.split(regime_data[target].values)):
            print(f"Processing {model_name} for fold {fold + 1} in {regime}")
            train, test = regime_data.iloc[train_index], regime_data.iloc[test_index]

            # Sequence generation
            generator = TimeseriesGenerator(train[target].values, train[target].values, length=sequence_length, batch_size=1)

            # Build LSTM/GRU Model
            if model_name == 'LSTM':
                model = Sequential([
                    LSTM(50, activation='relu', input_shape=(sequence_length, 1)),
                    Dense(1)
                ])
            else:
                model = Sequential([
                    GRU(50, activation='relu', input_shape=(sequence_length, 1)),
                    Dense(1)
                ])

            model.compile(optimizer='adam', loss='mse')
            model.fit(generator, epochs=20, verbose=1)

            # Predictions and Metrics
            test_generator = TimeseriesGenerator(test[target].values, test[target].values, length=sequence_length, batch_size=1)
            predictions = model.predict(test_generator).flatten()
            mse, rmse, mae, mape, r2, adj_r2 = calculate_metrics(test[target].values[sequence_length:], predictions, len(test) - sequence_length, len(features))
            var_95, es_95 = calculate_var_es(predictions, alpha=0.95)
            results_dict[model_name].append((mse, rmse, mae, mape, r2, adj_r2, var_95, es_95))

    # GBM Implementation for Each Regime
    for fold, (train_index, test_index) in enumerate(tscv.split(regime_data[target].values)):
        print(f"Processing GBM for fold {fold + 1} in {regime}")
        X_train, X_test = regime_data.iloc[train_index][features], regime_data.iloc[test_index][features]
        y_train, y_test = regime_data.iloc[train_index][target], regime_data.iloc[test_index][target]

        gbm = GradientBoostingRegressor()
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)

        gbm.fit(X_train_scaled, y_train)
        gbm_predictions = gbm.predict(X_test_scaled)
        mse, rmse, mae, mape, r2, adj_r2 = calculate_metrics(y_test, gbm_predictions, len(y_test), len(features))
        var_95, es_95 = calculate_var_es(gbm_predictions, alpha=0.95)
        results_dict['GBM'].append((mse, rmse, mae, mape, r2, adj_r2, var_95, es_95))

    regime_results[regime] = results_dict

# Compare Results Across Regimes
for regime, models in regime_results.items():
    print(f"Results for {regime} regime:")
    for model, metrics in models.items():
        avg_metrics = np.mean(metrics, axis=0)
        print(f"{model}: MSE={avg_metrics[0]:.2f}, RMSE={avg_metrics[1]:.2f}, MAE={avg_metrics[2]:.2f}, MAPE={avg_metrics[3]:.2f}, R²={avg_metrics[4]:.2f}, Adj R²={avg_metrics[5]:.2f}, VaR_95={avg_metrics[6]:.2f}, ES_95={avg_metrics[7]:.2f}")

"""## Regime Switch without macroeconomic variables"""

exclude_features = ['cpiret', 'UNRATE', 'GDP', 'cpiret_Lag1', 'UNRATE_Lag1', 'GDP_Lag1',
                    'cpiret_Lag2', 'UNRATE_Lag2', 'GDP_Lag2', 'cpiret_Lag3', 'UNRATE_Lag3', 'GDP_Lag3']
features = [col for col in data.columns if col not in exclude_features + ['PRC', 'date', 'TICKER', 'MarketRegime', 'MarkovRegime']]

# Threshold-Based Approach
data['MarketRegime'] = np.where(
    data['RET'] > 0.02, 'Bull',
    np.where(data['RET'] < -0.02, 'Bear', 'Crisis')
)

markov_model = MarkovRegression(data['RET'], k_regimes=3, trend='c', switching_variance=True)
markov_results = markov_model.fit()

data['MarkovRegime'] = markov_results.predict()

# Segment Data by Regime and Evaluate Model Performance
regime_results = {}
sequence_length = 10
n_splits = 2
target = 'PRC'

for regime in ['Bull', 'Bear', 'Crisis']:
    print(f"Evaluating models for {regime} regime")
    regime_data = data[data['MarketRegime'] == regime]

    # Prepare data for modeling
    tscv = TimeSeriesSplit(n_splits=n_splits)
    results_dict = {'LSTM': [], 'GRU': [], 'GBM': []}

    for model_name in ['LSTM', 'GRU']:
        for fold, (train_index, test_index) in enumerate(tscv.split(regime_data[target].values)):
            print(f"Processing {model_name} for fold {fold + 1} in {regime}")
            train, test = regime_data.iloc[train_index], regime_data.iloc[test_index]

            # Sequence generation
            generator = TimeseriesGenerator(train[target].values, train[target].values, length=sequence_length, batch_size=1)

            # Build LSTM/GRU Model
            if model_name == 'LSTM':
                model = Sequential([
                    LSTM(50, activation='relu', input_shape=(sequence_length, 1)),
                    Dense(1)
                ])
            else:
                model = Sequential([
                    GRU(50, activation='relu', input_shape=(sequence_length, 1)),
                    Dense(1)
                ])

            model.compile(optimizer='adam', loss='mse')
            model.fit(generator, epochs=20, verbose=1)

            # Predictions and Metrics
            test_generator = TimeseriesGenerator(test[target].values, test[target].values, length=sequence_length, batch_size=1)
            predictions = model.predict(test_generator).flatten()
            mse, rmse, mae, mape, r2, adj_r2 = calculate_metrics(test[target].values[sequence_length:], predictions, len(test) - sequence_length, len(features))
            var_95, es_95 = calculate_var_es(predictions, alpha=0.95)
            results_dict[model_name].append((mse, rmse, mae, mape, r2, adj_r2, var_95, es_95))

    # GBM Implementation for Each Regime
    for fold, (train_index, test_index) in enumerate(tscv.split(regime_data[target].values)):
        print(f"Processing GBM for fold {fold + 1} in {regime}")
        X_train, X_test = regime_data.iloc[train_index][features], regime_data.iloc[test_index][features]
        y_train, y_test = regime_data.iloc[train_index][target], regime_data.iloc[test_index][target]

        gbm = GradientBoostingRegressor()
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)

        gbm.fit(X_train_scaled, y_train)
        gbm_predictions = gbm.predict(X_test_scaled)
        mse, rmse, mae, mape, r2, adj_r2 = calculate_metrics(y_test, gbm_predictions, len(y_test), len(features))
        var_95, es_95 = calculate_var_es(gbm_predictions, alpha=0.95)
        results_dict['GBM'].append((mse, rmse, mae, mape, r2, adj_r2, var_95, es_95))

    regime_results[regime] = results_dict

# Compare Results Across Regimes
for regime, models in regime_results.items():
    print(f"Results for {regime} regime:")
    for model, metrics in models.items():
        avg_metrics = np.mean(metrics, axis=0)
        print(f"{model}: MSE={avg_metrics[0]:.2f}, RMSE={avg_metrics[1]:.2f}, MAE={avg_metrics[2]:.2f}, MAPE={avg_metrics[3]:.2f}, R²={avg_metrics[4]:.2f}, Adj R²={avg_metrics[5]:.2f}, VaR_95={avg_metrics[6]:.2f}, ES_95={avg_metrics[7]:.2f}")